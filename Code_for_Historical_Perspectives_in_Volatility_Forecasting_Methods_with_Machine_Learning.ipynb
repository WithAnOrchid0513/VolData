{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8I2xCn639Af"
      },
      "outputs": [],
      "source": [
        "!pip install keras-tuner\n",
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras_tuner as kt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN, GRU, LSTM, Dense, Dropout\n",
        "from keras.layers import *\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras import backend as K\n",
        "\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from sklearn.metrics import mean_absolute_error as mae\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he9lP9EyXATy"
      },
      "outputs": [],
      "source": [
        "# import data - 30 years from 1993/10/01 to 2023/10/01\n",
        "url = \"https://raw.githubusercontent.com/WithAnOrchid0513/VolData/main/SPY_data.csv\"\n",
        "df = pd.read_csv(url, index_col = 'Date', parse_dates = True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRdGUme2eMGX"
      },
      "outputs": [],
      "source": [
        "# stationary check with Augmented Dickeyâ€“Fuller test\n",
        "def adf(x):\n",
        "  res = adfuller(x)\n",
        "  print(\"Test-Statistic:\", res[0])\n",
        "  print(\"P-Value:\", res[1])\n",
        "  if res[1] < 0.05:\n",
        "    print(\"Stationary\")\n",
        "  else:\n",
        "    print(\"Non-Stationary\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXY47kVMeSgx"
      },
      "outputs": [],
      "source": [
        "# check close price\n",
        "adf(df.Close)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcD08xpGertH"
      },
      "outputs": [],
      "source": [
        "# calculate log returns\n",
        "df['log_returns'] = np.log(df.Close/df.Close.shift(1))\n",
        "df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MxQt3V9eYcp"
      },
      "outputs": [],
      "source": [
        "# check log returns\n",
        "adf(df['log_returns'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy2GCSPXGi1v"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ru9pXbi69T1a"
      },
      "outputs": [],
      "source": [
        "WINDOW_SIZE = 11\n",
        "# convert data to 22-day np array for LSTM prediction\n",
        "def windowed_dataset(x_series, y_series, lookback_window):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range((lookback_window-1), len(x_series)):\n",
        "        from_idx = x_series.index[i-lookback_window+1]\n",
        "        to_idx = x_series.index[i]\n",
        "        a = x_series[from_idx:to_idx].values\n",
        "        dataX.append(a)\n",
        "        dataY.append(y_series[to_idx])\n",
        "\n",
        "    return np.array(dataX), np.array(dataY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gezokMMq4kF0"
      },
      "outputs": [],
      "source": [
        "# calculate realized volatility\n",
        "def realized_volatility_daily(series_log_return):\n",
        "    return np.sqrt(np.sum(series_log_return**2)/(WINDOW_SIZE-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWcfvuBA47nh"
      },
      "outputs": [],
      "source": [
        "n_future = 22\n",
        "# get current volatility\n",
        "df['vol_current'] = df.log_returns.rolling(window=WINDOW_SIZE)\\\n",
        "                                   .apply(realized_volatility_daily)\n",
        "\n",
        "# get future volatility\n",
        "df['vol_future'] = df.log_returns.shift(n_future).rolling(window=WINDOW_SIZE)\\\n",
        "                                 .apply(realized_volatility_daily)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uC1rmPFuDwBX"
      },
      "outputs": [],
      "source": [
        "df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9fs0aaf5NqW"
      },
      "outputs": [],
      "source": [
        "# Base model\n",
        "#train test split, 27 years training, 3 years testing\n",
        "test_size = 504\n",
        "split_time_1 = 0\n",
        "split_time_2 = len(df) - test_size\n",
        "split_time_3 = 7521\n",
        "train_idx = df.index[split_time_1:split_time_2]\n",
        "test_idx = df.index[split_time_2:split_time_3]\n",
        "\n",
        "# Financial Crisis period\n",
        "# split_time_1 = 800\n",
        "# split_time_2 = split_time_1 + 2520\n",
        "# split_time_3 = split_time_2 + 504\n",
        "# train_idx = df.index[split_time_1:split_time_2]\n",
        "# test_idx = df.index[split_time_2:split_time_3]\n",
        "\n",
        "# COVID1 10yrs training\n",
        "# split_time_1 = 4088\n",
        "# split_time_2 = split_time_1 + 2520\n",
        "# split_time_3 = split_time_2 + 504\n",
        "# train_idx = df.index[split_time_1:split_time_2]\n",
        "# test_idx = df.index[split_time_2:split_time_3]\n",
        "\n",
        "# COVID2 15 yrs training\n",
        "# split_time_1 = 2828\n",
        "# split_time_2 = split_time_1 + 3780\n",
        "# split_time_3 = split_time_2 + 504\n",
        "# train_idx = df.index[split_time_1:split_time_2]\n",
        "# test_idx = df.index[split_time_2:split_time_3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksby8_qx6FkL"
      },
      "outputs": [],
      "source": [
        "print(f'Training \\tFrom: {train_idx[0]} \\tto {train_idx[-1]} \\t{len(train_idx)} days')\n",
        "print(f'Test \\t\\tFrom: {test_idx[0]} \\tto {test_idx[-1]} \\t{len(test_idx)} days')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTsbsoLt6QD0"
      },
      "outputs": [],
      "source": [
        "# split train test\n",
        "y_train = df.vol_future[train_idx]\n",
        "y_test = df.vol_future[test_idx]\n",
        "x_train = df.vol_current[train_idx]\n",
        "x_test = df.vol_current[test_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8j_QW27-7OBF"
      },
      "outputs": [],
      "source": [
        "# a plot of log_return and volatility\n",
        "plt.figure(figsize=(20,7))\n",
        "\n",
        "plt.plot(df.log_returns, color='gray', label='Daily Log Returns', alpha=0.4)\n",
        "\n",
        "plt.plot(y_train, color='blue', label='Training Volatility')\n",
        "plt.plot(y_test, color='green', label='Test Volatility')\n",
        "\n",
        "plt.plot()\n",
        "plt.title(f'TRAIN TEST SPLIT \\n(Daily Volatility Using {WINDOW_SIZE}-Day Interval)', fontsize=15)\n",
        "plt.legend()\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPzt5S4yFVyt"
      },
      "outputs": [],
      "source": [
        "# the forecasting function\n",
        "n_past = 22\n",
        "\n",
        "def val_forecast(model):\n",
        "    forecast = []\n",
        "    idx = df.index\n",
        "\n",
        "    for i in range(len(test_idx)):\n",
        "        # get the data at the previous n_past (22) time steps\n",
        "        from_idx = idx[split_time_2 + i - n_past + 1]\n",
        "        to_idx = idx[split_time_2 + i]\n",
        "        # make prediction\n",
        "        pred = model.predict(df.vol_current[from_idx:to_idx].values[np.newaxis])\n",
        "        forecast.append(pred)\n",
        "\n",
        "    forecast = np.array(forecast)[:, 0, 0]\n",
        "    preds = pd.Series(forecast, index=test_idx)\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pZPwiay8ZeZ"
      },
      "outputs": [],
      "source": [
        "# record model error\n",
        "perf_df = pd.DataFrame(columns=['Model', 'Validation RMSPE', 'Validation RMSE', 'Validation MAE'])\n",
        "def log_perf(y_true, y_pred, model_name):\n",
        "    perf_df.loc[len(perf_df.index)] = [model_name,\n",
        "                                       RMSPE(y_true, y_pred),\n",
        "                                       RMSE(y_true, y_pred),\n",
        "                                       MAE(y_true, y_pred)]\n",
        "    return perf_df\n",
        "\n",
        "# define Root Mean Squared Percentage Error function\n",
        "def RMSPE(y_true, y_pred):\n",
        "    output = np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
        "    return output\n",
        "\n",
        "\n",
        "# define Root Mean Squared Error function\n",
        "def RMSE(y_true, y_pred):\n",
        "    output = np.sqrt(mse(y_true, y_pred))\n",
        "    return output\n",
        "\n",
        "def MAE(y_true, y_pred):\n",
        "    output = mae(y_true, y_pred)\n",
        "    return output\n",
        "\n",
        "# rmspe for tensorflow\n",
        "def rmspe(y_true, y_pred):\n",
        "    output = ksqrt(kmean(ksquare((y_true - y_pred) / y_true)))\n",
        "    return output\n",
        "\n",
        "# plotting model predictions vs. target values\n",
        "def viz_model(y_true, y_pred, model_name):\n",
        "    plt.figure(figsize=(20,7))\n",
        "    plt.plot(y_true, color='blue', label='Real Volatility')\n",
        "    plt.plot(y_pred, color='orange', lw=3, label=f'Forecasted Volatility')\n",
        "\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Volatility')\n",
        "    plt.legend(loc='best');\n",
        "    plt.title(f'{model_name} \\non Test Data', fontsize=15)\n",
        "    plt.legend(loc='best');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_-cqNdOb4N-"
      },
      "outputs": [],
      "source": [
        "# these are depreciated in Keras\n",
        "def ksqrt(x):\n",
        "    zero = tf.convert_to_tensor(0.0, x.dtype)\n",
        "    x = tf.maximum(x, zero)\n",
        "    return tf.sqrt(x)\n",
        "\n",
        "def kmean(x, axis=None, keepdims=False):\n",
        "    if x.dtype.base_dtype == tf.bool:\n",
        "        x = tf.cast(x, backend.floatx())\n",
        "    return tf.reduce_mean(x, axis, keepdims)\n",
        "\n",
        "def ksquare(x):\n",
        "    return tf.square(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3wf-Mv673Xr"
      },
      "outputs": [],
      "source": [
        "# initialize for LSTM\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# reshape the data for LSTM\n",
        "mat_X_train, mat_y_train = windowed_dataset(x_train, y_train, n_past)\n",
        "mat_X_test, mat_y_test = windowed_dataset(x_test, y_test, n_past)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OHspMleqLqB"
      },
      "outputs": [],
      "source": [
        "# LSTM model\n",
        "def build_model(hp):\n",
        "    # Dropout setup\n",
        "    dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.3, step=0.1)\n",
        "\n",
        "    # a two layered model, with a potential third layer\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[None]),\n",
        "    ])\n",
        "\n",
        "    model.add(tf.keras.layers.Bidirectional(\n",
        "        tf.keras.layers.LSTM(\n",
        "            units=hp.Int('units_layer_1', min_value=64, max_value=128, step=32),\n",
        "            return_sequences=True)\n",
        "    ))\n",
        "    model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
        "\n",
        "    model.add(tf.keras.layers.Bidirectional(\n",
        "        tf.keras.layers.LSTM(\n",
        "            units=hp.Int('units_layer_2', min_value=32, max_value=64, step=32),\n",
        "            return_sequences=(hp.Int('num_layers', 2, 3) == 3)\n",
        "        )\n",
        "    ))\n",
        "    model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
        "\n",
        "    if hp.Int('num_layers', 2, 3) == 3:\n",
        "        model.add(tf.keras.layers.Bidirectional(\n",
        "            tf.keras.layers.LSTM(\n",
        "                units=hp.Int('units_layer_3', min_value=16, max_value=32, step=16))\n",
        "        ))\n",
        "        model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(),\n",
        "        loss='mse',\n",
        "        metrics=[rmspe]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# early stopping\n",
        "checkpoint_cb = ModelCheckpoint('lstm_1.keras', save_best_only=True, monitor='loss')\n",
        "early_stopping_cb = EarlyStopping(patience=20, restore_best_weights=True, monitor='val_rmspe', mode='min')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48Qt5l7WqHJG"
      },
      "outputs": [],
      "source": [
        "# hyperparameter search\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective=kt.Objective('val_rmspe', direction='min'),\n",
        "    max_trials=20,\n",
        "    executions_per_trial=5,\n",
        "    directory='lstm_tuning',\n",
        "    project_name='lstm'\n",
        ")\n",
        "\n",
        "tuner.search(\n",
        "    mat_X_train, mat_y_train,\n",
        "    epochs=10,\n",
        "    validation_split = 0.2,\n",
        "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHlR9Ez9phFY"
      },
      "outputs": [],
      "source": [
        "# record the best permutation\n",
        "lstm_best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "lstm_best_model = tuner.hypermodel.build(lstm_best_hps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YV-JKm-npmud"
      },
      "outputs": [],
      "source": [
        "print(f\"\"\"\n",
        "Best hyperparameters:\n",
        "units_layer_1: {lstm_best_hps.get('units_layer_1')}\n",
        "units_layer_2: {lstm_best_hps.get('units_layer_2')}\n",
        "dropout_rate: {lstm_best_hps.get('dropout_rate')}\n",
        "num_layers:{lstm_best_hps.get('num_layers')}\n",
        "\"\"\")\n",
        "if lstm_best_hps.get('num_layers') == 3:\n",
        "  print(f\"\"\"units_layer_3: {lstm_best_hps.get('units_layer_3')}\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6Fzq8Fb_mOT"
      },
      "outputs": [],
      "source": [
        "# fit LSTM\n",
        "lstm_res = lstm_best_model.fit(mat_X_train, mat_y_train, epochs=200, validation_data=(mat_X_test, mat_y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaZAmI_ZC54S"
      },
      "outputs": [],
      "source": [
        "# predict\n",
        "lstm_preds = val_forecast(lstm_best_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDsNi50mWART"
      },
      "outputs": [],
      "source": [
        "# annualize\n",
        "y_test = y_test * np.sqrt(252)\n",
        "lstm_preds = lstm_preds * np.sqrt(252)\n",
        "\n",
        "# store result\n",
        "log_perf(y_test, lstm_preds, 'LSTM')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUKBA8uHiV72"
      },
      "outputs": [],
      "source": [
        "#make graph\n",
        "viz_model(y_test, lstm_preds, \"2-Layered LSTM\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivMc5tAeBzDv"
      },
      "outputs": [],
      "source": [
        "# show model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(lstm_best_model, show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18Jvp6_XGcSP"
      },
      "source": [
        "# Implied Volatility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NLDBRKZxLXU"
      },
      "outputs": [],
      "source": [
        "# load vix as implied volatility data\n",
        "vix_url = \"https://raw.githubusercontent.com/WithAnOrchid0513/VolData/main/VIX_data.csv\"\n",
        "vix = pd.read_csv(vix_url, index_col = 'DATE', parse_dates = True)\n",
        "vix.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQsgH2j6n6qg"
      },
      "outputs": [],
      "source": [
        "iv = vix.CLOSE[test_idx] * 0.01\n",
        "iv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAxyCyTboNOG"
      },
      "outputs": [],
      "source": [
        "# store result\n",
        "log_perf(y_test, iv, 'Implied Volatility')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKNDD1fHqzNN"
      },
      "outputs": [],
      "source": [
        "# make graph\n",
        "viz_model(y_test, iv, \"Implied Volatility\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUHSPY7OGPeH"
      },
      "source": [
        "# GARCH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1MbV8TDrECk"
      },
      "outputs": [],
      "source": [
        "pip install arch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3hr6tEsxQlG"
      },
      "outputs": [],
      "source": [
        "from arch import arch_model\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "pd.options.mode.chained_assignment = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4nYB811xPRN"
      },
      "outputs": [],
      "source": [
        "# rolling forecast\n",
        "rolling_forecasts = []\n",
        "idx = df.index\n",
        "\n",
        "# iterate over each time step in the validation set\n",
        "for i in range(len(test_idx)+n_future):\n",
        "\n",
        "    if i < n_future:\n",
        "      idx = train_idx[-i-1]\n",
        "    else:\n",
        "      idx = test_idx[i-n_future]\n",
        "    # scale the data for GARCH convergance issue\n",
        "    train = df.log_returns[:idx] * 100\n",
        "\n",
        "    model = arch_model(train, vol='GARCH', p=1, q=1,\n",
        "                       dist='normal')\n",
        "    model_fit = model.fit(disp='off')\n",
        "\n",
        "    # make predictions\n",
        "    vaR = model_fit.forecast(horizon=n_future,\n",
        "                             reindex=False).variance.values\n",
        "    pred = np.sqrt(np.mean(vaR))\n",
        "\n",
        "    rolling_forecasts.append(pred)\n",
        "\n",
        "gm_preds = pd.Series(rolling_forecasts, index=df.index[split_time_2-n_future:split_time_3])\n",
        "# annualize and unscale the data\n",
        "gm_preds = gm_preds * np.sqrt(252) / 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i397GWgMECH7"
      },
      "outputs": [],
      "source": [
        "gm_preds = gm_preds.shift(n_future)\n",
        "gm_preds = gm_preds.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ifngwaTZlmI"
      },
      "outputs": [],
      "source": [
        "# store result\n",
        "log_perf(y_test, gm_preds, 'GARCH(1,1)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1meE-Rb0Lz5"
      },
      "outputs": [],
      "source": [
        "# make graph\n",
        "viz_model(y_test, gm_preds, 'GARCH(1,1)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk4_Gh180AKJ"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dehuh3BkRAgs"
      },
      "outputs": [],
      "source": [
        "# reshape the data\n",
        "transformer_x_train, transformer_y_train = windowed_dataset(x_train, y_train, n_past)\n",
        "transformer_x_test, transformer_y_test = windowed_dataset(x_test, y_test, n_past)\n",
        "\n",
        "transformer_x_train = transformer_x_train.reshape((transformer_x_train.shape[0], transformer_x_train.shape[1], 1))\n",
        "transformer_x_test = transformer_x_test.reshape((transformer_x_test.shape[0], transformer_x_test.shape[1], 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2SmhdpYRCX3"
      },
      "outputs": [],
      "source": [
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(inputs, inputs)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    return x + res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTJRS968sCrW"
      },
      "outputs": [],
      "source": [
        "def build_model(hp):\n",
        "    input_shape = transformer_x_train.shape[1:]\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = inputs\n",
        "\n",
        "    # Hyperparameters to tune\n",
        "    head_size = hp.Choice('head_size', values=[64, 128, 256])\n",
        "    num_heads = hp.Choice('num_heads', values=[2, 4, 8])\n",
        "    ff_dim = hp.Choice('ff_dim', values=[4, 8, 16, 32])\n",
        "    num_transformer_blocks = hp.Choice('num_transformer_blocks', values=[4, 8])\n",
        "    mlp_units = hp.Int('mlp_units', min_value=64, max_value=256, step=64)\n",
        "    dropout = hp.Float('dropout', 0.1, 0.3, step=0.1)\n",
        "    mlp_dropout = hp.Float('mlp_dropout', 0.1, 0.3, step=0.1)\n",
        "\n",
        "    # Build Transformer Blocks\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    x = layers.Dense(mlp_units, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = layers.Dense(1, activation=\"linear\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    model.compile(\n",
        "        loss='mse',\n",
        "        optimizer=keras.optimizers.Adam(),\n",
        "        metrics=[rmspe]\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9_y1YT-tiMK"
      },
      "outputs": [],
      "source": [
        "# hyperparameter search\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective=kt.Objective('val_rmspe', direction='min'),\n",
        "    max_trials=20,\n",
        "    executions_per_trial=5,\n",
        "    directory='transformer_tuning',\n",
        "    project_name='transformer'\n",
        ")\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uc5Yx59lvLdo"
      },
      "outputs": [],
      "source": [
        "tuner.search(\n",
        "    transformer_x_train,\n",
        "    transformer_y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=10,\n",
        "    callbacks=callbacks,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gu8XQqYlvPdE"
      },
      "outputs": [],
      "source": [
        "# record the best permutation\n",
        "transformer_best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"\"\"Best hyperparameters:\n",
        "head_size: {transformer_best_hps.get('head_size')}\n",
        "num_heads: {transformer_best_hps.get('num_heads')}\n",
        "ff_dim: {transformer_best_hps.get('ff_dim')}\n",
        "num_transformer_blocks: {transformer_best_hps.get('num_transformer_blocks')}\n",
        "mlp_units: {transformer_best_hps.get('mlp_units')}\n",
        "dropout: {transformer_best_hps.get('dropout')}\n",
        "mlp_dropout: {transformer_best_hps.get('mlp_dropout')}\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcwnHfg7vZqa"
      },
      "outputs": [],
      "source": [
        "model = tuner.hypermodel.build(transformer_best_hps)\n",
        "\n",
        "# fit Transformer\n",
        "transformer_res = model.fit(\n",
        "    transformer_x_train,\n",
        "    transformer_y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=200,\n",
        "    callbacks=callbacks,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isL25hVHRQGN"
      },
      "outputs": [],
      "source": [
        "# predict\n",
        "t_preds = val_forecast(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZG1fqF3RShO"
      },
      "outputs": [],
      "source": [
        "t_preds = t_preds * np.sqrt(252)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwsxGYk2RVDH"
      },
      "outputs": [],
      "source": [
        "# make graph\n",
        "viz_model(y_test, t_preds, \"Transformer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvWW_PWFRXa0"
      },
      "outputs": [],
      "source": [
        "# store result\n",
        "log_perf(y_test, t_preds, 'Transformer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oc8QJnkrzaWq"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(4, 1, figsize=(20, 18))\n",
        "\n",
        "# labels\n",
        "labels = ['Realized Volatility', 'Implied Volatility', 'GARCH(1,1) Predicted Volatility', 'LSTM Predicted Volatility', 'Transformer Predicted Volatility']\n",
        "\n",
        "# First plot: Real vs Implied Volatility\n",
        "axs[0].plot(y_test, color='blue', label=labels[0])\n",
        "axs[0].plot(iv, color='purple', label=labels[1])\n",
        "axs[0].set_title('Realized vs. Implied Volatility', fontsize=20)\n",
        "axs[0].legend(fontsize=20)\n",
        "\n",
        "# Second plot: Real vs GARCH(1,1) Forecasted Volatility\n",
        "axs[1].plot(y_test, color='blue', label=labels[0])\n",
        "axs[1].plot(gm_preds, color='orange', label=labels[2])\n",
        "axs[1].set_title('Real vs. GARCH(1,1) Predicted', fontsize=20)\n",
        "axs[1].legend(fontsize=20)\n",
        "\n",
        "# Third plot: Real vs LSTM Forecasted Volatility\n",
        "axs[2].plot(y_test, color='blue', label=labels[0])\n",
        "axs[2].plot(lstm_preds, color='green', label=labels[3])\n",
        "axs[2].set_title('Real vs. LSTM Predicted', fontsize=20)\n",
        "axs[2].legend(fontsize=20)\n",
        "\n",
        "# Fourth plot: Real vs Transformer Forecasted Volatility\n",
        "axs[3].plot(y_test, color='blue', label=labels[0])\n",
        "axs[3].plot(t_preds, color='red', label=labels[4])\n",
        "axs[3].set_title('Real vs. Transformer Predicted w/ 15 yrs training', fontsize=20)\n",
        "axs[3].legend(fontsize=20)\n",
        "\n",
        "for ax in axs:\n",
        "    ax.set_xlabel('Time', fontsize=20)\n",
        "    ax.set_ylabel('Volatility', fontsize=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQktLaTdvqDC"
      },
      "source": [
        "# Additional models: GRU and RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUZqnRaajh68"
      },
      "outputs": [],
      "source": [
        "#y_test = y_test * np.sqrt(252)\n",
        "gru_X_train = mat_X_train.reshape((mat_X_train.shape[0], mat_X_train.shape[1], 1))\n",
        "gru_X_test = mat_X_test.reshape((mat_X_test.shape[0], mat_X_test.shape[1], 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCUpvvjCL4Rk"
      },
      "outputs": [],
      "source": [
        "def build_gru_model(hp):\n",
        "    model = Sequential()\n",
        "    # Dropout setup\n",
        "    dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.3, step=0.1)\n",
        "\n",
        "    for i in range(hp.Int('num_layers', 1, 3)):\n",
        "        return_sequences = i < hp.Int('num_layers', 1, 3) - 1\n",
        "        model.add(GRU(\n",
        "            units=hp.Int(f'gru_units_{i}', min_value=32, max_value=128, step=32),\n",
        "            return_sequences=return_sequences\n",
        "        ))\n",
        "        model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
        "\n",
        "    model.add(Dense(hp.Int('dense_units', 16, 128, step=16), activation='relu'))\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(),\n",
        "        loss='mse',\n",
        "        metrics=[rmspe]\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AChMtW5OddC"
      },
      "outputs": [],
      "source": [
        "tuner = kt.RandomSearch(\n",
        "    build_gru_model,\n",
        "    objective=kt.Objective('val_rmspe', direction='min'),\n",
        "    max_trials=20,\n",
        "    executions_per_trial=5,\n",
        "    directory='gru_tuning',\n",
        "    project_name='gru_hp'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhUFe0znQnvK"
      },
      "outputs": [],
      "source": [
        "tuner.search(gru_X_train, mat_y_train,\n",
        "             validation_split = 0.2,\n",
        "             epochs=10,\n",
        "             callbacks=[EarlyStopping(patience=5)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jH3-TUlR815"
      },
      "outputs": [],
      "source": [
        "gru = tuner.get_best_models(num_models=1)[0]\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(\"Best Hyperparameters:\")\n",
        "for key in best_hps.values.keys():\n",
        "    print(f\"{key}: {best_hps.get(key)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnsh_cO-b8WO"
      },
      "outputs": [],
      "source": [
        "gru.fit(gru_X_train, mat_y_train, validation_split=0.2, epochs=200, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bArLaqyPcMRF"
      },
      "outputs": [],
      "source": [
        "gru_preds = val_forecast(gru)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQkFw8v9cQ4c"
      },
      "outputs": [],
      "source": [
        "gru_preds = gru_preds * np.sqrt(252)\n",
        "\n",
        "# store result\n",
        "log_perf(y_test, gru_preds, 'GRU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6xayla5cUok"
      },
      "outputs": [],
      "source": [
        "viz_model(y_test, gru_preds, \"GRU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7kD_LA2uVR2"
      },
      "outputs": [],
      "source": [
        "rnn_X_train = mat_X_train.reshape((mat_X_train.shape[0], mat_X_train.shape[1], 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1ow5hg0db3U"
      },
      "outputs": [],
      "source": [
        "def build_rnn_model(rnn_units=32, dropout_rate=0.2, output_dim=1):\n",
        "    model = Sequential([\n",
        "        SimpleRNN(rnn_units, return_sequences=False),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(output_dim)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=[rmspe])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPIfwAdNdetU"
      },
      "outputs": [],
      "source": [
        "rnn = build_rnn_model()\n",
        "rnn.fit(rnn_X_train, mat_y_train, validation_split=0.2, epochs=200, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GG6sf4Hhdn-y"
      },
      "outputs": [],
      "source": [
        "rnn_preds = val_forecast(rnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqgWn-_SgRpQ"
      },
      "outputs": [],
      "source": [
        "rnn_preds = rnn_preds * np.sqrt(252)\n",
        "\n",
        "# store result\n",
        "log_perf(y_test, rnn_preds, 'RNN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDM3uwSogbfJ"
      },
      "outputs": [],
      "source": [
        "viz_model(y_test, gru_preds, \"RNN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOkHboK8gkTK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
